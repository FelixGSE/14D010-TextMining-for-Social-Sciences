{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, party, speech_text, stopwords, clean_length):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.party = party\n",
    "        self.text = speech_text.lower()\n",
    "        self.word_list(clean_length, stopwords)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "    \n",
    "    def word_list(self, clean_length, stopwords):\n",
    "        \"\"\"\n",
    "        description: define the word_list attribute (i.e. without stemming)\n",
    "        \"\"\"\n",
    "        self.word_list = np.array(wordpunct_tokenize(self.text))\n",
    "        self.word_list = np.array([t for t in self.word_list if (t.isalpha() and len(t) > clean_length)])        \n",
    "        self.word_list = np.array([t for t in self.word_list if t not in stopwords])\n",
    "\n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    # We added in the initialization the dictionary with respect to which you want to compute the ranking: \n",
    "    # numb is for the number of top documents you want to consider, while metric == \"tfidf\" or \"doc-term\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length, pres_parties):\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "                \n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = []\n",
    "        for doc in doc_data:\n",
    "            pres_last_name = doc[1]\n",
    "            party = pres_parties[pres_last_name] if pres_last_name in pres_parties.keys() else None\n",
    "            new_doc = Document(doc[0], pres_last_name, party, doc[2], self.stopwords, clean_length)\n",
    "            self.docs.append(new_doc)\n",
    "            \n",
    "        # sort docs by political party\n",
    "        groups = defaultdict(list)\n",
    "        for obj in self.docs:\n",
    "            groups[obj.party].append(obj)\n",
    "        new_list = groups.values()\n",
    "        self.docs = [item for sublist in new_list for item in sublist]\n",
    "                \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length < 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "\n",
    "    # Q1 part 1: document_term_matrix - which returns a D by V array of frequency counts.            \n",
    "    def generate_document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create the document_term_matrix\n",
    "        \"\"\"\n",
    "        dimD = self.N\n",
    "        # total number of columns\n",
    "        dimV = len(self.token_set)\n",
    "        terms_list = list(self.token_set)\n",
    "        # initialize the matrix\n",
    "        document_term_matrix = np.zeros((dimD, dimV))        \n",
    "        for i in range(dimD):\n",
    "            # count the terms for each document\n",
    "            document = self.docs[i]\n",
    "            if i%25==0: print 'counting terms for doc: ' + str(i)\n",
    "            word_counts = Counter(document.tokens)\n",
    "            for word_count_pair in word_counts.most_common():\n",
    "                # split in word and count\n",
    "                word = word_count_pair[0]\n",
    "                count = word_count_pair[1]\n",
    "                # save the term index\n",
    "                term_idx = terms_list.index(word)\n",
    "                doc_term_tuple = (i, term_idx)\n",
    "                document_term_matrix.itemset(doc_term_tuple, count)\n",
    "        # update the doc_term_matrix attribute        \n",
    "        self.document_term_matrix = document_term_matrix\n",
    "\n",
    "    def generate_idfv(self):\n",
    "        \"\"\"\n",
    "        computes the inverse document frequency of each term v\n",
    "        \"\"\"\n",
    "        D = self.N\n",
    "        # idf_{v} = log(D/d_fv)\n",
    "        terms_list = list(self.token_set)\n",
    "        self.idfv = dict.fromkeys(terms_list, 0)\n",
    "        # creates a hash {'term':0,'term2':0,...}\n",
    "        for v in self.token_set:\n",
    "            term_idx = terms_list.index(v)\n",
    "            d_fv = np.sum(self.document_term_matrix[:,term_idx] > 0)\n",
    "            # apply the formula\n",
    "            c = math.log10(D/d_fv)\n",
    "            self.idfv[v] = c\n",
    "\n",
    "    # Q1 part 2: tf_idf - returns a D by V array of tf-idf scores            \n",
    "    def generate_tf_idf(self):\n",
    "        D = self.N\n",
    "        terms_list = list(self.token_set)\n",
    "        # initialize the matrix\n",
    "        tf_idf = np.zeros(self.document_term_matrix.shape)\n",
    "        for doc_i in range(D):\n",
    "            if doc_i%25==0: print 'counting terms for doc: ' + str(doc_i)\n",
    "            for v_i in range(len(terms_list)):\n",
    "                doc_term_tuple = (doc_i, v_i)\n",
    "                xdv_score = self.document_term_matrix.item(doc_term_tuple)\n",
    "                if xdv_score > 0:\n",
    "                    idf_score = self.idfv[terms_list[v_i]]\n",
    "                    if idf_score > 0:\n",
    "                        # apply the formula seen in class\n",
    "                        tf_idf_score = (1 + np.log(xdv_score))*idf_score\n",
    "                        tf_idf.itemset(doc_term_tuple, tf_idf_score)\n",
    "        self.tf_idf = tf_idf\n",
    "\n",
    "    # Q1 part 3: dict_rank\n",
    "    def dict_rank(self, numb, dictionary, metric):\n",
    "        \"\"\"\n",
    "        computes the dictionary rank of the top numb documents, based on the dictionary and metric method \n",
    "        (either doc-term or tfidf)\n",
    "        \"\"\"\n",
    "        terms_list = list(self.token_set)\n",
    "        # get the indices of occurences of the terms in the dictionary\n",
    "        idcs = [terms_list.index(item) for item in dictionary]\n",
    "        order = []\n",
    "        if metric == 'doc-term':\n",
    "            # get the needed columns\n",
    "            cols = tuple([list(self.document_term_matrix[:,i]) for i in idcs])\n",
    "            # sort and update order list\n",
    "            order = list(np.lexsort(cols))\n",
    "            order.reverse()\n",
    "        elif metric == 'tf_idf':\n",
    "            cols = tuple([list(self.tf_idf[:,i]) for i in idcs])\n",
    "            order = list(np.lexsort(cols))\n",
    "            order.reverse()\n",
    "        # return the top numb documents    \n",
    "        return [self.docs[i] for i in order[0:numb]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('../Week1HW/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wilson': 'Democratic', 'Jackson': 'Democratic', 'Buren': 'Democratic', 'Reagan': 'Republican', 'Pierce': 'Democratic', 'Bush': 'Republican', 'Tyler': 'Whig', 'Coolidge': 'Republican', 'Hoover': 'Republican', 'Harding': 'Republican', 'Grant': 'Republican', 'Ford': 'Republican', 'Eisenhower': 'Republican', 'Obama': 'Democratic', 'Lincoln': 'Republican/National Union', 'Adams': 'Federalist', 'Johnson': 'Democratic/National Union', 'Kennedy': 'Democratic', 'Roosevelt': 'Republican', 'Hayes': 'Republican', 'Arthur': 'Republican', 'Taft': 'Republican', 'Henry': 'Whig', 'Clinton': 'Democratic', 'Nixon': 'Republican', 'Madison': 'Democratic-Republican', 'Taylor': 'Whig', 'Fillmore': 'Whig', 'Carter': 'Democratic', 'Buchanan': 'Democratic', 'Washington': 'Independent', 'Garfield': 'Republican', 'Jefferson': 'Democratic-Republican', 'Harrison': 'Republican', 'Cleveland': 'Democratic', 'McKinley': 'Republican', 'Truman': 'Democratic', 'Polk': 'Democratic', 'Monroe': 'Democratic-Republican'}\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "pres_metadata = genfromtxt('pres_metadata.tsv', delimiter='\\t', names = True, dtype= None)\n",
    "\n",
    "pres_parties = {}\n",
    "for i in range(len(pres_metadata)):\n",
    "    last_name = pres_metadata[i]['Last']\n",
    "    if not last_name in pres_parties.keys():\n",
    "        pres_parties[last_name] = pres_metadata[i]['Party']\n",
    "\n",
    "print pres_parties\n",
    "print len(pres_parties.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fellow-citizens of the s\n"
     ]
    }
   ],
   "source": [
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, '../Week1HW/stopwords.txt', 3, pres_parties)\n",
    "print corpus.docs[0].text[0:25]\n",
    "\n",
    "# CHECK OUR WORK\n",
    "# for i in range(len(corpus.docs)):\n",
    "#     print corpus.docs[i].pres + ' is affliated with the ' + str(corpus.docs[i].party) + ' party.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting terms for doc: 0\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 75\n",
      "counting terms for doc: 100\n",
      "counting terms for doc: 125\n",
      "counting terms for doc: 150\n",
      "counting terms for doc: 175\n",
      "counting terms for doc: 200\n",
      "counting terms for doc: 225\n",
      "time spent computing document_term_matrix: 207.569051981\n",
      "13588\n",
      "counting terms for doc: 0\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 75\n",
      "counting terms for doc: 100\n",
      "counting terms for doc: 125\n",
      "counting terms for doc: 150\n",
      "counting terms for doc: 175\n",
      "counting terms for doc: 200\n",
      "counting terms for doc: 225\n",
      "(236, 13588)\n"
     ]
    }
   ],
   "source": [
    "# Q2 part 1: Use the two methods above to score each document in your data.\n",
    "import time\n",
    "t0 = time.time()\n",
    "corpus.generate_document_term_matrix()\n",
    "corpus.document_term_matrix[:,0]\n",
    "t1 = time.time()\n",
    "print 'time spent computing document_term_matrix: ' + str(t1 - t0)\n",
    "\n",
    "corpus.generate_idfv()\n",
    "print len(corpus.idfv) # == len(corpus.token_set)\n",
    "corpus.idfv[corpus.idfv.keys()[9]]\n",
    "\n",
    "corpus.generate_tf_idf()\n",
    "print corpus.tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "[ 1.          0.1233673   0.18749003  0.14247549  0.15269546  0.09685395\n",
      "  0.14338882  0.06745857  0.09074177  0.08064543  0.07242745  0.09601532\n",
      "  0.06775577  0.05627785  0.06394946  0.10218228  0.05368801  0.07093068\n",
      "  0.11474534  0.09960175  0.09058679  0.09385505  0.09817714  0.07407106\n",
      "  0.0764705   0.05144827  0.05447246  0.05345622  0.05936839  0.07613458\n",
      "  0.05887399  0.09685397  0.10188679  0.06026158  0.12372114  0.08063384\n",
      "  0.09591824  0.06774948  0.04540258  0.06054393  0.06423356  0.04044441\n",
      "  0.06296118  0.06406253  0.0229444   0.03158374  0.03619887  0.02405588\n",
      "  0.02494243  0.01328416  0.07125239  0.06149412  0.082294    0.08113806\n",
      "  0.04858756  0.08631996  0.08308064  0.07267689  0.05697599  0.05620938\n",
      "  0.05867019  0.05545621  0.04412697  0.06630099  0.05566819  0.05044021\n",
      "  0.07832019  0.06203478  0.07824163  0.06831105  0.06636182  0.04313458\n",
      "  0.0470978   0.05258295  0.05894993  0.04512392  0.04886972  0.04499137\n",
      "  0.04542274  0.05776664  0.05979084  0.05436285  0.05573709  0.05058851\n",
      "  0.05703192  0.05628568  0.04290741  0.04893401  0.05484696  0.04083519\n",
      "  0.03780188  0.05419187  0.04957871  0.05754547  0.03438794  0.02666169\n",
      "  0.03706433  0.02804775  0.03319241  0.0454661   0.03874731  0.02691551\n",
      "  0.02810461  0.01564774  0.02666015  0.04058629  0.0129111   0.01441329\n",
      "  0.0299784   0.01692506  0.01763136  0.01493046  0.01465257  0.02340893\n",
      "  0.01660391  0.01453392  0.02170156  0.03139285  0.03168091  0.04250499\n",
      "  0.02971996  0.03117944  0.02736015  0.0116642   0.01818609  0.02428353\n",
      "  0.02731749  0.02989499  0.0220007   0.03037196  0.01556253  0.01477283\n",
      "  0.03221997  0.01278345  0.02692424  0.02038316  0.03394923  0.01870644\n",
      "  0.03519654  0.01648014  0.01514892  0.01878879  0.01628338  0.02542425\n",
      "  0.01425834  0.01682198  0.03293308  0.02713517  0.01660593  0.0102463\n",
      "  0.00863293  0.01592393  0.01207629  0.01739539  0.02037996  0.01646299\n",
      "  0.02709587  0.02685269  0.03204744  0.03714462  0.07101624  0.05573965\n",
      "  0.06542901  0.07033447  0.07008035  0.10167918  0.07854557  0.10056666\n",
      "  0.07288705  0.0991019   0.10816015  0.11949354  0.09969826  0.10222392\n",
      "  0.11418268  0.08854613  0.07429163  0.07604042  0.09785085  0.069114\n",
      "  0.08146312  0.10281789  0.06336477  0.06149152  0.07623216  0.0815176\n",
      "  0.08626581  0.06822596  0.07054205  0.06754344  0.03767447  0.05712456\n",
      "  0.05298866  0.05812907  0.06978832  0.06503396  0.03135018  0.03149601\n",
      "  0.04069698  0.03137125  0.02352508  0.03748873  0.01800576  0.03414344\n",
      "  0.03367164  0.01984509  0.01523564  0.02725085  0.01337304  0.00815861\n",
      "  0.01359017  0.0194761   0.01322799  0.03340641  0.01513251  0.02874468\n",
      "  0.02009735  0.00759081  0.02781418  0.02380241  0.0268047   0.03254431\n",
      "  0.02075323  0.01623905  0.03340163  0.01824092  0.01392415  0.02280147\n",
      "  0.02135695  0.02288805  0.01319177  0.01649786  0.01485786  0.01379481\n",
      "  0.01623357  0.02701692]\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def gen_cosine_matrix(corpus, doc_term_matrix):\n",
    "    cosine_matrix = np.zeros((corpus.N, corpus.N))\n",
    "    for i in range(corpus.N):\n",
    "        for j in range(corpus.N):\n",
    "            if i == j:\n",
    "                cosine_matrix[i,j] = 1\n",
    "            elif j < i:\n",
    "                cosine_matrix[i,j] = cosine_matrix[j,i]\n",
    "            else:\n",
    "                cosine_matrix[i,j] = 1-scipy.spatial.distance.cosine(doc_term_matrix[i,:], doc_term_matrix[j,:])\n",
    "    return cosine_matrix\n",
    "\n",
    "cosine_matrix = gen_cosine_matrix(corpus, corpus.tf_idf)\n",
    "# test\n",
    "print cosine_matrix.item((1,2)) == cosine_matrix.item((2,1))\n",
    "print cosine_matrix.item((2,2)) == 1\n",
    "print cosine_matrix.item((2,1)) != cosine_matrix.item((2,3))\n",
    "\n",
    "print cosine_matrix[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('aimeeb', '***')\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=cosine_matrix\n",
    "    )\n",
    "]\n",
    "plot_url = py.plot(data, filename='basic-sou-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><a href=\"https://plot.ly/~aimeeb/4/\" target=\"_blank\" title=\"\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~aimeeb/4.png\" alt=\"\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https://plot.ly/404.png';\" /></a><script data-plotly=\"aimeeb:4\"  src=\"https://plot.ly/embed.js\" async></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<div><a href=\"https://plot.ly/~aimeeb/4/\" target=\"_blank\" title=\"\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~aimeeb/4.png\" alt=\"\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src=\\'https://plot.ly/404.png\\';\" /></a><script data-plotly=\"aimeeb:4\"  src=\"https://plot.ly/embed.js\" async></script></div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236, 236)\n",
      "(236, 236)\n",
      "(13588, 13588)\n"
     ]
    }
   ],
   "source": [
    "#svd_sou = np.linalg.svd(corpus.tf_idf)\n",
    "a = svd_sou[0]\n",
    "s = svd_sou[1]\n",
    "b = svd_sou[2]\n",
    "\n",
    "print a.shape\n",
    "print np.diag(s).shape\n",
    "print b.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least eigens required: 129\n",
      "(236, 13588)\n"
     ]
    }
   ],
   "source": [
    "# Use svd\n",
    "p = 0.7\n",
    "sum_of_all_eigens = np.sum(s)\n",
    "k = 0\n",
    "for i in range(len(s)):\n",
    "    res = np.sum(s[0:i])/sum_of_all_eigens\n",
    "    if res > p:\n",
    "        k = i\n",
    "        break\n",
    "        \n",
    "print 'least eigens required: ' + str(k)\n",
    "tf_idf_pca = np.dot(np.dot(a[:,0:k], np.diag(s[0:k])), b.T[0:k,:])\n",
    "\n",
    "print tf_idf_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.79340641,  0.80664352,  0.79139459,  0.82479415,\n",
       "        0.73520092,  0.84537699,  0.79806626,  0.74929426,  0.73928675,\n",
       "        0.75063101,  0.75335834,  0.72889676,  0.70036866,  0.76342647,\n",
       "        0.78453582,  0.76789793,  0.73918473,  0.75268488,  0.72559368,\n",
       "        0.73405246,  0.6762715 ,  0.80700697,  0.74008296,  0.64674075,\n",
       "        0.56623183,  0.62150008,  0.61402679,  0.53434132,  0.63229825,\n",
       "        0.56780259,  0.5991937 ,  0.72195463,  0.71136501,  0.74318345,\n",
       "        0.77931075,  0.43790163,  0.42123541,  0.34678048,  0.174538  ,\n",
       "        0.28017565,  0.43519217,  0.24312688,  0.25066939,  0.13837608,\n",
       "        0.25970026,  0.20192327,  0.15869352,  0.16777973,  0.16485001,\n",
       "        0.26827476,  0.28999707,  0.46450658,  0.34189316,  0.19094515,\n",
       "        0.4479368 ,  0.35950945,  0.30673148,  0.46142122,  0.22582278,\n",
       "        0.40947371,  0.22477763,  0.19357067,  0.2381464 ,  0.2126672 ,\n",
       "        0.31159697,  0.40365334,  0.41443143,  0.32278155,  0.28353908,\n",
       "        0.26304357,  0.16768437,  0.19545213,  0.21207499,  0.24255124,\n",
       "        0.16718762,  0.19067898,  0.17611629,  0.17045891,  0.23067216,\n",
       "        0.23380506,  0.21663439,  0.2243844 ,  0.19936039,  0.2255625 ,\n",
       "        0.22896425,  0.16797187,  0.19423916,  0.21857197,  0.16038014,\n",
       "        0.1478531 ,  0.21615348,  0.19607087,  0.22814587,  0.12619634,\n",
       "        0.12381738,  0.22546009,  0.17454168,  0.13225694,  0.20060393,\n",
       "        0.1544322 ,  0.13191593,  0.10833762,  0.14870974,  0.14557583,\n",
       "        0.21765412,  0.25254538,  0.31161434,  0.23989388,  0.28309181,\n",
       "        0.17184358,  0.24526529,  0.21578306,  0.25886243,  0.16460015,\n",
       "        0.03417315,  0.12838106,  0.12918906,  0.15978219,  0.2196266 ,\n",
       "        0.18176915,  0.1783721 ,  0.10404627,  0.16610973,  0.27695171,\n",
       "        0.16621558,  0.20677513,  0.10315412,  0.07169686,  0.21668398,\n",
       "        0.18602616,  0.18729392,  0.12656942,  0.14539999,  0.09926172,\n",
       "        0.16297453,  0.1415931 ,  0.10466612,  0.18863189,  0.15980203,\n",
       "        0.07643042,  0.10008894,  0.06558014,  0.11172948,  0.03486265,\n",
       "        0.08649371,  0.14303478,  0.12272183,  0.05897792,  0.09954575,\n",
       "        0.11205503,  0.05564932,  0.06537877,  0.08946513,  0.08975745,\n",
       "        0.04151904,  0.10242573,  0.11095839,  0.14058531,  0.14881865,\n",
       "        0.35783718,  0.23059386,  0.47340297,  0.299048  ,  0.3349003 ,\n",
       "        0.4288055 ,  0.64144863,  0.7553034 ,  0.63719238,  0.41229768,\n",
       "        0.46677243,  0.53432279,  0.41143436,  0.44480483,  0.47943742,\n",
       "        0.3525966 ,  0.27222167,  0.30629895,  0.407771  ,  0.26752624,\n",
       "        0.32685397,  0.43764053,  0.24418266,  0.24735943,  0.30959776,\n",
       "        0.33256715,  0.34910799,  0.26631782,  0.2811964 ,  0.27206033,\n",
       "        0.27405138,  0.2342672 ,  0.21303627,  0.23395766,  0.28087589,\n",
       "        0.25765554,  0.33995157,  0.36515864,  0.18468625,  0.40716519,\n",
       "        0.24631294,  0.14645217,  0.28991393,  0.37736167,  0.1340689 ,\n",
       "        0.21973203,  0.23014941,  0.17737399,  0.17726129,  0.19777844,\n",
       "        0.12098044,  0.06822134,  0.03256805,  0.15345012,  0.04818352,\n",
       "        0.1503532 ,  0.08374071,  0.14091614,  0.1117061 ,  0.18650497,\n",
       "        0.10481969,  0.12999622,  0.06096454,  0.06989995,  0.13999837,\n",
       "        0.08338745,  0.05637746,  0.08734612,  0.08235962,  0.09147083,\n",
       "        0.07103296,  0.06482319,  0.05740981,  0.05338246,  0.06362607,\n",
       "        0.11387261])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_pca = gen_cosine_matrix(corpus, tf_idf_pca)\n",
    "cosine_similarity_pca[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "py.sign_in('aimeeb', '***')\n",
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=cosine_similarity_pca\n",
    "    )\n",
    "]\n",
    "plot_url = py.plot(data, filename='pca-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots urls:\n",
    "* [basic cosine](https://plot.ly/~aimeeb/4)\n",
    "* [pca cosine](https://plot.ly/~aimeeb/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><a href=\"https://plot.ly/~aimeeb/2/\" target=\"_blank\" title=\"\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~aimeeb/2.png\" alt=\"\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https://plot.ly/404.png';\" /></a><script data-plotly=\"aimeeb:2\"  src=\"https://plot.ly/embed.js\" async></script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<div><a href=\"https://plot.ly/~aimeeb/2/\" target=\"_blank\" title=\"\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~aimeeb/2.png\" alt=\"\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src=\\'https://plot.ly/404.png\\';\" /></a><script data-plotly=\"aimeeb:2\"  src=\"https://plot.ly/embed.js\" async></script></div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
