{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re   \n",
    "import json\n",
    "import numpy as np\n",
    "import time, random\n",
    "import os\n",
    "\n",
    "# Get URLs from starting page\n",
    "def get_url_list( url ):\n",
    "    # Read URL and create bs object\n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    # Look for all td tags on the website\n",
    "    td_list = soup.find_all(\"td\")\n",
    "    # Initialize list for href\n",
    "    href_table = []\n",
    "    # Filter relevant tags\n",
    "    for td in td_list:\n",
    "        try:\n",
    "            if td['class'][0] == 'title': \n",
    "                temp = td.find_all(\"a\")[0]['href']\n",
    "                href_table.append(temp)\n",
    "        except:\n",
    "            None\n",
    "    # Return URLs for each poem\n",
    "    return href_table\n",
    "\n",
    "# Read poem title from URL\n",
    "def get_titles( urls ):\n",
    "    titles = map(lambda x: re.sub('[/-]',' ',x[6:])[:-1],urls)\n",
    "    return titles\n",
    "\n",
    "# Read poems from a URL list\n",
    "def get_poems( url_list , max_delay = None ):\n",
    "    poems = []\n",
    "    for ind in url_list:\n",
    "        if max_delay is not None:\n",
    "            sleep_time = random.randint(1,max_delay)\n",
    "            time.sleep(sleep_time)\n",
    "        url = 'http://www.poemhunter.com/' + ind\n",
    "        html = urllib.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "        div = soup.find_all(\"div\")\n",
    "        for dv in div:\n",
    "            try:\n",
    "                if dv['class'][0] == 'KonaBody':\n",
    "                    temp = dv.find_all(\"p\")[0]\n",
    "                    temp = clean(temp)\n",
    "                    poems.append(temp)\n",
    "            except:\n",
    "                None\n",
    "    return poems\n",
    "\n",
    "# Convert poems to unicode and replace html tags\n",
    "def clean( item, replacements = [\"<p>\",\"</p>\",\"<br>\",\"<br/>\"] ):\n",
    "    item = item.prettify()\n",
    "    for rep in replacements:\n",
    "        item = item.replace(unicode(rep),\"\")\n",
    "    return item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawl_poems( url , file_name = \"poem\", start = 1, end = 10, sleep = None, log = 25 ):\n",
    "    \n",
    "    # Initialize log_counter and empty poem list\n",
    "    log_counter = 0\n",
    "    poems = []\n",
    "    \n",
    "    # Start scraping\n",
    "    for i in range( start , end ):\n",
    "        \n",
    "        # Show trace\n",
    "        print \"Iteration: \" + str( i ) + \" of \" + str( end ) \n",
    "        \n",
    "        # Add page counter \n",
    "        temp_url = url + str( i )\n",
    "        \n",
    "        temp_url_list = get_url_list( temp_url )\n",
    "        temp_titles = get_titles( temp_url_list )\n",
    "        temp_poems = get_poems( temp_url_list, max_delay = sleep )\n",
    "        temp_result = zip( temp_titles , temp_poems )\n",
    "        \n",
    "        # Update poem list\n",
    "        poems.extend( temp_result )\n",
    "        \n",
    "        ### HERE SHOULD BE A LOG \n",
    "        \n",
    "        # Update log counter and save data to HD \n",
    "        log_counter = log_counter + 1\n",
    "        \n",
    "        if log_counter == log:\n",
    "            \n",
    "            log_file = file_name + str( i ) + '.txt'\n",
    "            with open(log_file, 'w') as backup:\n",
    "                json.dump(poems, backup)\n",
    "                \n",
    "            # Reset log counter\n",
    "            log_counter = 0\n",
    "            # Re-initialize poems list\n",
    "            poems = []\n",
    "            # Print log trace\n",
    "            print(\"I did a backup in iteration: \" + str(i) )\n",
    "    \n",
    "    # If data left - Save to HD\n",
    "    if log_counter != log:\n",
    "        log_file = file_name + str( i ) + '.txt'\n",
    "        with open(log_file, 'w') as backup:\n",
    "            json.dump(poems, backup)\n",
    "    \n",
    "    # Print final statement    \n",
    "    print \"I'm done!!! Iteration: \" + str(i) + \" of \" + str(end) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 of 5\n",
      "Iteration: 2 of 5\n",
      "I did a backup in iteration: 2\n",
      "Iteration: 3 of 5\n",
      "Iteration: 4 of 5\n",
      "I did a backup in iteration: 4\n",
      "I'm done!!! Iteration: 4 of 5\n"
     ]
    }
   ],
   "source": [
    "# DEMO\n",
    "\n",
    "# Plug in your working directory\n",
    "os.chdir(\"/Users/felix/Downloads\")\n",
    "\n",
    "# Choose starting web adress\n",
    "start_url = 'http://www.poemhunter.com/poems/new-poems/?a=0&l=new&order=submitted&p='\n",
    "\n",
    "# Get poems\n",
    "crawl_poems(url = start_url , start = 1 , end = 5, sleep = 4, log = 2 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
