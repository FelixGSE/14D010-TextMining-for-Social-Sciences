{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf460
{\fonttbl\f0\fmodern\fcharset0 Courier;\f1\fmodern\fcharset0 Courier-Bold;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;\red101\green71\blue146;\red148\green6\blue75;
\red19\green36\blue126;\red14\green114\blue164;\red58\green62\blue68;\red254\green254\blue254;\red0\green0\blue0;
\red41\green67\blue135;}
\paperw11900\paperh16840\margl1440\margr1440\vieww19600\viewh12020\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3384\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3384\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\partightenfactor0

\f1\b \expnd0\expndtw0\kerning0
\ul Homework: 
\f0\b0 \ulnone \
\
For this homework we chose to scrape poemhunter.com, which is a website that collects many different kind of poems (either famous ones and amatorial).\
In particular, we focused on the \'93new poems\'94 section, where new amatorial short poems are continuously published, because we thought it would be more interesting.\
As a general note, we tried to maintain the original structure of the poems, thus preserving line breaks and erasing html tags (more details later on).\
\

\f1\b \ul Function-Manual:
\f0\b0 \ulnone \

\itap1\trowd \taflags0 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clshdrawnil \clwWidth18560\clftsWidth3 \clmart10 \clmarl10 \clmarb10 \clmarr10 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadl200 \clpadr200 \gaph\cellx8640
\pard\intbl\itap1\tx566\tx1133\tx1700\tx2267\tx2834\tx3384\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\partightenfactor0
\cf3 \
crawl_poems\cf2 (url , file_name \cf4 =\cf2  \cf5 "poem"\cf2 , start \cf4 =\cf2  \cf6 1\cf2 , end \cf4 =\cf2  \cf6 10\cf2 , sleep \cf4 =\cf2  \cf6 None\cf2 , log \cf4 =\cf2  \cf6 25\cf2  )\cell \lastrow\row
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3384\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\partightenfactor0
\cf0 \
Function arguments:\
\
- url: the url you want to start from (this for sure works with the \'93new poems\'94 section, but we did not check others);\
\
- file_name: the name of the file the poems will be saved into;\
\
- start: the previous url is such that it contains different pages; \'91start\'92 is an integer which flags for the number of page you want to start from, so it completes the previous url;\
\
- end: last page you want to consider in the previous url;\
\
- sleep: integer, upper bound on the number of seconds you want to randomly wait for scraping each page;\
\
- log: frequency of the dump in number of pages (every \'91log\'92 number of pages you save the scraped poems in a .txt).\
\

\f1\b \ul Code-Description:
\f0\b0 \ulnone \
\
The code follows the following procedure. Each page in the 
\f2\fs24 \cf2 \'93
\f0\fs28 \cf0 new poem\'94 section points to 25 poems. First we get the urls for each poem from each page and use them to accuses the poems. We continue this procedure for each page. \
\
In more detail the code consists of three main functions and two auxiliary functions:\
\
\cf7 \cb8 \shad\shadx0\shady-20\shadr0\shado0 \shadc9 We first use the function 
\f2\fs24 \cf3 \cb1 \shad0 get_url_list\cf2 () 
\f0\fs28 \cf7 \cb8 \shad\shadx0\shady-20\shadr0\shado0 \shadc9 to read all URLs from the current page by filtering the relevant tags. From this first function, we get each poem's URL from the current page. \
\

\f2\fs24 \cf9 \cb1 \shad0 We feed the list of gathered urls to the \cf3 get_poems\cf2 () function, which scrapes the actual poem from this url. 
\f0\fs28 \cf7 \cb8 \shad\shadx0\shady-20\shadr0\shado0 \shadc9 The function notably contains an option for random sleeping time between the scraping of two poems.\
\
The function 
\f2\fs24 \cf3 \cb1 \shad0 crawl_poems\cf2 () is a wrapper function using the former two functions to crawl all poems from all page in the section \'93new poems\'94 (arguments see previous section). Furthermore it saves periodically all poems with corresponding title to the hard drive. 
\f0\fs28 \cf7 \cb8 \shad\shadx0\shady-20\shadr0\shado0 \shadc9 \
\
Finally we added two auxiliary functions. The function 
\f2\fs24 \cf3 \cb1 \shad0 get_titles\cf2 () extracts the title of a poem from an url. On the other hand the \cf3 clean\cf2 () function processes the crawled poems by removing html tags, but preserving line breaks.\

\f0\fs28 \cf0 \

\f1\b \cf9 \ul \ulc9 Robustness:\ulnone  
\f0\b0 \
\
\shad\shadx0\shady-20\shadr0\shado0 \shadc9 To ensure that our crawler doesn't get stuck at a wrong URL, we implemented a try/except chunk in the \'91
\f2\fs24 \cf3 \shad0 get_poems\cf2 ()
\f0\fs28 \cf9 \shad\shadx0\shady-20\shadr0\shado0 \shadc9  function\'92. The crawler then tries to get a URL, and if not, it just continues. The random wait option can play the same kind of role when pages do not load sufficiently fast.\
\
The log is notably used within the try/except chunk. If the try is a success, the URL is added to a list of successfully loaded URLs. Otherwise, it is added to the list of unloaded URLs.\
\
Moreover, the crawler comes with unit tests for all its low-level functions. If changing the source code, ensure tests still pass using the command 
\f1\b \'93python test_crawler.py\'94
\f0\b0 .\
 \
Finally, we did not observe any restrictions from {\field{\*\fldinst{HYPERLINK "http://l.facebook.com/l.php?u=http%3A%2F%2Fpoemhunter.com%2Frobots.txt&h=6AQELxNP9"}}{\fldrslt \ul \ulc10 \shad\shadx0\shady-20\shadr0\shado0 \shadc9 poemhunter.com/robots.txt}}.\shad0 \
\
\
}