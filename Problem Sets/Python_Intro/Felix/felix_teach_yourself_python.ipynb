{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 0 HW2\n",
    "\n",
    "1. This assignment is an individual effort.\n",
    "2. Submission to be uploaded into your group repositories in the folder python_intro\n",
    "3. Deadline is 20th of April 5:00 PM.\n",
    "4. Please follow google's [python styleguide](https://google.github.io/styleguide/pyguide.html) for your code. Pay attention to the guidelines for naming convention, comments and main.\n",
    "5. Code will be checked for plagiarism. Compelling signs of a duplicated effort will lead to a rejection of submission and will attract a 100\\% grade penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "Please load the file provided to you by email. Use _json_ module to read it as a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 use open a connection to the file\n",
    "# 2 read contents of the file\n",
    "# 3 use the json module to read the string as a list\n",
    "#(replace None with relevant code)\n",
    "\n",
    "### MY CODE \n",
    "\n",
    "# Import Jason lib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(\"/Users/felix/Documents/GSE/Term 3/14D010 Text Mining for Social Sciences/14D010-TextMining-for-Social-Sciences/Python_Intro/Felix\")\n",
    "\n",
    "file_handle = open(\"1929_Hoover_felix.txt\") \n",
    "file_content = file_handle.read()\n",
    "speech = json.loads(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that type(speech) is list. Please take a moment to go through the python list documentation and check out the various ways to manipulate lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2\n",
    "The first element of the list is the year of speech, the second element is the president who gave it, while the third is the transcript of the same. \n",
    "\n",
    "1. Inspect the transcript. Note the commonly used non-alphanumerical characters. Use an appropriate method of strings to get rid of them.\n",
    "2. Use an appropriate string method to split the string of the speech into a list of smaller list of words.\n",
    "3. Convert all words into lower case and return the list. Use a for loop. Then use a list comprehension to do the same.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MY CODE\n",
    "\n",
    "# Import regular expresssion\n",
    "import re\n",
    "\n",
    "# Define speech variable\n",
    "speech_text = speech[2]\n",
    "# Clean non-alphanum. char from speech list\n",
    "stripped_text = re.sub('[^0-9a-zA-Z]+', ' ', speech_text)\n",
    "# Split text by words\n",
    "word_list = stripped_text.split()\n",
    "\n",
    "# Create list of lower words with a generic for loop\n",
    "lower_words = []\n",
    "for words in word_list:\n",
    "    lower_words.append(words.lower())\n",
    "\n",
    "# Repeat with list comprehension    \n",
    "lower_words=[words.lower() for words in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'to', u'the', u'senate', u'and', u'house', u'of', u'representatives', u'the', u'constitution', u'requires']\n"
     ]
    }
   ],
   "source": [
    "# Show output chunk\n",
    "print(lower_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\n",
    "Create a function _preprocess_ that takes as arguments _text_ and _non_alphanum_, a string of non-alphanumeric characters that you want get rid of. Perform all operations specfied in the previous question. However, converting to lowercase should be an optional argument. The data structure returned should be a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### MY CODE\n",
    "\n",
    "# Import regular expression\n",
    "import re\n",
    "\n",
    "# READ ME: Note that the function expects the alphanum to be formated as '[Your String Pattern]'\n",
    "\n",
    "# Define preprocess function \n",
    "def preprocess(text, alphanum_chars, lower = True):\n",
    "    # Clean word list\n",
    "    stripped = re.sub(alphanum_chars,' ', text)\n",
    "    # Create a list of words\n",
    "    word_list = stripped.split()\n",
    "    # Return output according to bool lower\n",
    "    if lower == True:\n",
    "        lower_words=[words.lower() for words in word_list]\n",
    "        return lower_words\n",
    "    else:\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "Create a function _word_freq_ that takes as input a word list that has been preprocessed and returns a dictionary of the word frequency. Which is the fourth most frequent word of your word list? (Provide code that computes it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'to', 341)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MY CODE\n",
    "\n",
    "# Initialize word freq function\n",
    "def wordfreq( word_list ):\n",
    "    # Creat dictonary with word counts\n",
    "    count = {words:word_list.count(words) for words in word_list}\n",
    "    # Return output\n",
    "    return count\n",
    "\n",
    "### Demo\n",
    "\n",
    "# Use preprocess function to process speech text\n",
    "pre_words = preprocess( speech_text,'[^0-9a-zA-Z]+',lower=False)\n",
    "\n",
    "# Count words and show fourth most common word \n",
    "frequencies = wordfreq(pre_words)\n",
    "\n",
    "# Sort dictionary - type: List with tuples\n",
    "sorted(frequencies.items(), key=lambda x: x[1],reverse=True)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\n",
    "Write a function that takes as input a word list and returns a dictionary of the frequencies of word lengths. Do not use the api collections for this assignment. But have a look at its [documentation](https://docs.python.org/2/library/collections.html). Its useful tool to have in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 247, 2: 2107, 3: 2265, 4: 1394, 5: 1022, 6: 862, 7: 806, 8: 737, 9: 610, 10: 412, 11: 295, 12: 210, 13: 99, 14: 75, 15: 10, 16: 3, 17: 1, 18: 1, 20: 1, 22: 1, 24: 1}\n"
     ]
    }
   ],
   "source": [
    "### MY CODE\n",
    "\n",
    "# Initialize wordlength function\n",
    "def wordlength(string_list):\n",
    "    # Map over word list and find length\n",
    "    word_len = map(len,string_list)\n",
    "    # Count frequency and create corresponding dictionary\n",
    "    count = {length:word_len.count(length) for length in word_len}\n",
    "    # Return output\n",
    "    return count\n",
    "\n",
    "### Demo \n",
    "\n",
    "# Show word length count\n",
    "print(wordlength(pre_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\n",
    "Load the file _sou_all.txt_ in ./data/pres_speech. Inspect its contents. Familiarise yourself with using regular expressions in python. You can use this [document](https://docs.python.org/2/howto/regex.html) as a starting point. Now use regular expressions to seperate the different speeches. Your function should accept the text and a regular expression as input and return a list of lists. Each element of the list should be a list with following structure:\n",
    "\n",
    "1. year\n",
    "2. president\n",
    "3. List of the transcript of the speech broken down into paragraphs.\n",
    "\n",
    "Save your result as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MY CODE\n",
    "\n",
    "# Read File \n",
    "file_handle = open(\"sou_all.txt\") \n",
    "file_content = file_handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define function to separate the data according to pre defined patters\n",
    "def separate_speeches( data, regex_list ):\n",
    "    \n",
    "    # Initilize split patterns from regex list\n",
    "    pattern01 = re.compile( regex_list[0] )\n",
    "    pattern02 = re.compile( regex_list[1] )\n",
    "    pattern03 = re.compile( regex_list[2] )\n",
    "    pattern04 = re.compile( regex_list[3] )\n",
    "    pattern05 = re.compile( regex_list[4] )\n",
    "    \n",
    "    # Do first split - separate each speech and corresponding president etc.\n",
    "    speeches = pattern01.split( file_content )[1:]\n",
    "    \n",
    "    # Initialize final list and begin separating each speech \n",
    "    final_list = []\n",
    "    for speech in speeches:\n",
    "    \n",
    "        # Separate temporary speech from meta info, s.a. president, year etc.\n",
    "        temp_split = pattern02.split( speech ) \n",
    "     \n",
    "    # Step 1) Process the spech\n",
    "        \n",
    "        # Select current speech\n",
    "        temp_speech = temp_split[1]\n",
    "        # Split by paragraph using paragraph pattern\n",
    "        temp_speech_split = pattern03.split( temp_speech )[1:-2]\n",
    "       \n",
    "    # Step 2) Process meta data \n",
    "    \n",
    "        # Select meta info\n",
    "        temp_title = temp_split[0]\n",
    "        # Split the meta data in year, speech type and president - \n",
    "        # Ignore first element, because == []\n",
    "        temp_title_split = pattern04.split( temp_title, 3 )[1:]\n",
    "        # Clean up year and president according to specified patterns\n",
    "        temp_year = re.sub( pattern05, '' , temp_title_split[0] )\n",
    "        temp_president = re.sub( pattern04, ' ', temp_title_split[2] )[:-1]\n",
    "        \n",
    " \n",
    "    # Step 3) Combine information into final list and append data\n",
    "        \n",
    "        temp_final_item = [ temp_year , temp_president , temp_speech_split ]\n",
    "        final_list.append( temp_final_item )\n",
    "     \n",
    "    # Return final output and end function\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Demo\n",
    "\n",
    "# After inspecting data define pattern list\n",
    "pattern_list = [\"\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\",\"\\*\\*\\*\\*\\*\",\"\\n\\n\" ,\"\\_\" ,\"\\*\"]\n",
    "\n",
    "# Run function and clean speeches\n",
    "speeches_clean = separate_speeches( file_content, pattern_list) \n",
    "\n",
    "# Export data\n",
    "with open(\"speeches.json\", 'w') as speeches_jason:\n",
    "    json.dump( speeches_clean, speeches_jason, indent = 3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
