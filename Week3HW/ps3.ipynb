{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.99104426988e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = np.random.beta(0.1, 1)\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I was offered a beer the other day that was reportedly made with citra hops. What are citra hops? Why should I care that my beer is made with them?', 'As far as we know, when did humans first brew beer, and where? Around when would you have been able to get your hands on something resembling a modern lager?', \"How is low/no alcohol beer made? I'm assuming that the beer is made normally and the alcohol is then removed, is it any more than just boiling it off? I've noticed that no/low alcohol beers' taste improved hugely a few years ago, is this due to a new technique?\", 'Citra is a registered trademark since 2007. Citra Brand hops have fairly high alpha acids and total oil contents with a low percentage of cohumulone content and  imparts interesting citrus and tropical fruit characters to beer.\\n\\nFor more information, you can read the Wikipedia article on the Citra brand.', \"In general, what's the best way to work out the temperature at which to serve a particular beer? Room temperature? Cold? Supercold? Warm?\", 'Currently I am storing my bottles in the crates at a (about) 20 degree angle (bottles are upwards!).\\n\\nDoes the way of storing the bottles affect something (and how)?', \"Assuming we're comparing equivalent amounts of alcohol, do certain beers get you inebriated more quickly or slowly? Does the amount of fizz make a difference?\", 'Apart from coming out of different taps, some ales seem very similar to lagers (although there are clearly a much greater variety of ales). Is there a difference in the way they are made?', \"It's pretty cold at the moment. Mulled wine being more of a Christmas drink, mulled beer is getting popular. What beers, and what spices should I use to make it?\", \"I usually drink strong Belgian Ales, particularly Triples, Quads and Trappists, so I'm no stranger to strong beer.  But I've noticed that I get far, far worse hangovers when drinking IPAs.  \\n\\nIs there anything about IPAs that would make this possible?  The lower quality places like ask.com or Yahoo answers usually say no, that only ABV produces hangovers, though one source did seem to imply that IPAs have special ingredients that make this a possibility. \\n\\nSo I want to ask the experts here: do IPAs have ingredients that other strong beers lack, that could exacerbate hangovers? \"]\n"
     ]
    }
   ],
   "source": [
    "from xml.etree import cElementTree as ET\n",
    "import sys\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "# credit: http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "posts = open('Posts.xml', 'r').read()\n",
    "\n",
    "posts[1:100]\n",
    "\n",
    "def remove_tags(text):\n",
    "    return ''.join(ET.fromstring(text).itertext())\n",
    "\n",
    "root = ET.fromstring(posts)\n",
    "documents = []\n",
    "for child in root.findall('row')[0:100]:\n",
    "    text = None\n",
    "    child_text = child.get('Body').encode('utf-8').strip()\n",
    "    text = strip_tags(child_text)\n",
    "    documents.append(text)\n",
    "\n",
    "print documents[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'wa' u'offer' u'beer' u'the' u'day' u'wa' u'reportedli' u'citra' u'hop'\n",
      " u'are' u'citra' u'hop' u'whi' u'care' u'beer']\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "execfile('corpus.py')\n",
    "execfile('document.py')\n",
    "\n",
    "corpus = Corpus(documents, '../Week1HW/stopwords.txt', 3)\n",
    "\n",
    "print corpus.docs[0].tokens\n",
    "# FIXME: Not sure why this is not 100\n",
    "print corpus.N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "1. Randomly assign all words to a topic\n",
    "2. Initialize theta (D x K, document-specific topic probabilities): each theta_d is a k-dimensional sample from Dir(alpha + number of words in topic k)\n",
    "3. Initialize beta (K x V, topic-specific term probabilities): each beta_k is a V-dimensional sample from Dir(eta + number of times term v appeared in topic k)\n",
    "\n",
    "#### Repeat\n",
    "\n",
    "1. for every word wi in N, it's current allocation is zi\n",
    "    1.1. decrement -1 zi document-topic count and topic-term count\n",
    "    (Question) Do we update theta, beta for these reduced counts?\n",
    "    1.2. Use theta and beta to calculate probability over topic for word (slide 14 lecture6)\n",
    "    1.3. draw from multinomial with these probabilites - this is the new allocation zi for word wi\n",
    "    1.4. increment +1 zi document-topic count and topic-term count\n",
    "2. update theta and beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: skunk\n",
      "Original topic distribution: [ 6.  2.  2.]\n",
      "Term: can\n",
      "Original topic distribution: [ 26.  15.  19.]\n",
      "Term: wine\n",
      "Original topic distribution: [ 3.  8.  3.]\n",
      "Term: saison\n",
      "Original topic distribution: [ 0.  3.  1.]\n",
      "Iteration 0\n",
      "[ 0.3715127   0.24354962  0.38493769]\n",
      "[ 0.00521869  0.00727849  0.00084276]\n",
      "[  8.96499578e-05   6.08596577e-05   2.61853725e-05]\n",
      "[ 0.0454536   0.04743505  0.04645351]\n",
      "[ 0.06644518  0.0594474   0.07233897]\n",
      "[ 0.00030227  0.00131574  0.00094881]\n",
      "[ 0.00521869  0.00727849  0.00084276]\n",
      "[ 0.          0.          0.00090331]\n",
      "[ 0.00345078  0.00022846  0.00143478]\n",
      "[ 0.00123987  0.00335368  0.00385033]\n",
      "[ 0.01296966  0.01946525  0.01812061]\n",
      "[ 0.00345078  0.00022846  0.00143478]\n",
      "[ 0.00123987  0.00335368  0.00385033]\n",
      "[ 0.00446502  0.00351178  0.00181186]\n",
      "[  3.93915273e-04   9.62839212e-05   0.00000000e+00]\n",
      "[ 0.0454536   0.04743505  0.04645351]\n",
      "Iteration 25\n",
      "Iteration 50\n",
      "Iteration 75\n",
      "Term: skunk\n",
      "New topic distribution: [ 10.   0.   0.]\n",
      "Term: can\n",
      "New topic distribution: [ 23.   0.  37.]\n",
      "Term: wine\n",
      "New topic distribution: [  0.  14.   0.]\n",
      "Term: saison\n",
      "New topic distribution: [ 0.  4.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "#\n",
    "D = len(corpus.docs)\n",
    "K = 3 # K - numer of topics\n",
    "V = len(corpus.token_set)\n",
    "\n",
    "# Randomly assign each word in each document to a topic\n",
    "# each array is of \"arbitrary\" length - length of document's respective word list\n",
    "word_topic_assignments = [[]]*corpus.N\n",
    "for doci, doc in enumerate(corpus.docs):\n",
    "    word_topic_assignments[doci] = []\n",
    "    for wordi, word in enumerate(doc.tokens):\n",
    "        word_topic_assignments[doci].append(np.random.randint(0,K))\n",
    "\n",
    "#print word_topic_assignments[0]\n",
    "\n",
    "# Initialize document counts of words for each topic, D x K\n",
    "document_topic_word_distribution = np.zeros((D, K))\n",
    "for doci, doc in enumerate(corpus.docs):\n",
    "    # sum the number of words in topic k\n",
    "    for k in range(0,K):\n",
    "        document_topic_word_distribution.itemset((doci, k), word_topic_assignments[doci].count(k))\n",
    "\n",
    "#print document_topic_word_distribution[0,:]\n",
    "\n",
    "# Initialize term topic counts, e.g. number of times term V was allocated to topic K, K x V\n",
    "# For every word in every document, determine what it's term index is\n",
    "topic_term_distribution = np.zeros((K, V))\n",
    "termlist = list(corpus.token_set)\n",
    "for doci, doc in enumerate(corpus.docs):\n",
    "    for wordi, word in enumerate(doc.tokens):\n",
    "        termidx = termlist.index(word)\n",
    "        topic_alloc = word_topic_assignments[doci][wordi]\n",
    "        current_count = topic_term_distribution.item((topic_alloc,termidx))\n",
    "        topic_term_distribution.itemset((topic_alloc,termidx), current_count + 1)\n",
    "        \n",
    "#print topic_term_distribution[0,:][0:10]\n",
    "\n",
    "alpha = 50/K\n",
    "eta = 200/V\n",
    "\n",
    "# Initialize theta - document-specific topic probabilities\n",
    "theta = np.zeros((D, K))\n",
    "# draw theta - Dirichlet with paramters (alpha + n_{d,k}) - num of words in doc with topic alloc k\n",
    "for doci in range(D):\n",
    "    theta_params = alpha + document_topic_word_distribution[doci,:]\n",
    "    # theta for this document\n",
    "    theta_d = np.random.dirichlet(tuple(theta_params), 1)\n",
    "    theta[doci,:] = theta_d\n",
    "\n",
    "# Initialize beta - x\n",
    "beta = np.zeros((K, V))\n",
    "# draw beta - Dirichlet with paramters (eta + m_{k,v}) - num of times term appeared in topic k\n",
    "for k in range(K):\n",
    "    beta_params = eta + topic_term_distribution[k,:]\n",
    "    beta_k = np.random.dirichlet(tuple(beta_params), 1)\n",
    "    beta[k,:] = beta_k\n",
    "    \n",
    "terms_to_sanity_check = ['skunk', 'can', 'wine', 'saison']\n",
    "for term in terms_to_sanity_check:\n",
    "    termidx = termlist.index(term)    \n",
    "    print 'Term: ' + term\n",
    "    print 'Original topic distribution: ' + str(topic_term_distribution[:,termidx])\n",
    "\n",
    "iters = 100\n",
    "for i in range(iters):\n",
    "    if i%25 == 0: print 'Iteration ' + str(i)\n",
    "    # 1 iteration\n",
    "    for doci, doc in enumerate(corpus.docs):\n",
    "        theta_d = theta[doci,:]\n",
    "        if i == 0 and doci == 0: print theta_d\n",
    "        for wordi, word in enumerate(doc.tokens):\n",
    "            word_topic_alloc = word_topic_assignments[doci][wordi]\n",
    "            termidx = termlist.index(word)\n",
    "            beta_v = beta[:,termidx]\n",
    "            if i == 0 and doci == 0: print beta_v            \n",
    "            # decrement counts\n",
    "            current_doc_count = document_topic_word_distribution.item((doci, word_topic_alloc))\n",
    "            document_topic_word_distribution.itemset((doci, word_topic_alloc), current_doc_count - 1)\n",
    "            current_topic_count = topic_term_distribution.item((word_topic_alloc, termidx))\n",
    "            topic_term_distribution.itemset((word_topic_alloc, termidx), current_topic_count - 1)\n",
    "            probs = theta_d*beta_v/np.sum(theta_d*beta_v)\n",
    "            new_topic_alloc = np.random.multinomial(1, tuple(probs))\n",
    "            new_topic_alloc = list(new_topic_alloc).index(1)                \n",
    "            word_topic_assignments[doci][wordi] = new_topic_alloc\n",
    "            # increment counts\n",
    "            current_doc_count = document_topic_word_distribution.item((doci, new_topic_alloc))\n",
    "            document_topic_word_distribution.itemset((doci, new_topic_alloc), current_doc_count + 1)\n",
    "            current_topic_count = topic_term_distribution.item((new_topic_alloc, termidx))\n",
    "            topic_term_distribution.itemset((new_topic_alloc, termidx), current_topic_count + 1) \n",
    "    # update theta\n",
    "    for doci in range(D):\n",
    "        theta_params = alpha + document_topic_word_distribution[doci,:]\n",
    "        # theta for this document\n",
    "        theta_d = np.random.dirichlet(tuple(theta_params), 1)\n",
    "        theta[doci,:] = theta_d\n",
    "    for k in range(K):\n",
    "        beta_params = eta + topic_term_distribution[k,:]\n",
    "        beta_k = np.random.dirichlet(tuple(beta_params), 1)\n",
    "        beta[k,:] = beta_k    \n",
    "        \n",
    "for termi, term in enumerate(terms_to_sanity_check):\n",
    "    termidx = termlist.index(term)\n",
    "    print 'Term: ' + term\n",
    "    print 'New topic distribution: ' + str(topic_term_distribution[:,termidx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most likely terms for topic 0:\n",
      "for\n",
      "the\n",
      "beer\n",
      "alcohol\n",
      "use\n",
      "can\n",
      "ale\n",
      "ani\n",
      "stout\n",
      "lager\n",
      "how\n",
      "gener\n",
      "brew\n",
      "glass\n",
      "out\n",
      "much\n",
      "caus\n",
      "plastic\n",
      "mani\n",
      "higher\n",
      "\n",
      "Most likely terms for topic 1:\n",
      "and\n",
      "are\n",
      "you\n",
      "the\n",
      "bottl\n",
      "thi\n",
      "drink\n",
      "yeast\n",
      "will\n",
      "ipa\n",
      "all\n",
      "abv\n",
      "becaus\n",
      "wine\n",
      "keep\n",
      "brewer\n",
      "imperi\n",
      "carbon\n",
      "depend\n",
      "gluten\n",
      "\n",
      "Most likely terms for topic 2:\n",
      "the\n",
      "beer\n",
      "but\n",
      "ferment\n",
      "not\n",
      "can\n",
      "differ\n",
      "temperatur\n",
      "tast\n",
      "wa\n",
      "get\n",
      "flavor\n",
      "process\n",
      "ha\n",
      "exampl\n",
      "good\n",
      "may\n",
      "age\n",
      "don\n",
      "doe\n"
     ]
    }
   ],
   "source": [
    "# Print the most likely words for every topic\n",
    "sorted_beta = np.zeros(beta.shape)\n",
    "\n",
    "for k in range(0,K):\n",
    "    print ''\n",
    "    print 'Most likely terms for topic ' + str(k) + ':'\n",
    "    sorted_beta[k,:] = np.argsort(beta[k,:])[::-1]\n",
    "    for i in range(20):\n",
    "        idx = int(sorted_beta[k,:][i])\n",
    "        print termlist[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
