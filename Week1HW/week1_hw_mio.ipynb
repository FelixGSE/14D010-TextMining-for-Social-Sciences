{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    # I added in the initialization the dictionary with respect to which you want to compute the ranking \n",
    "    # (and numb for the number of top documents you want to consider), and method == \"tdidf\" or \"doc-term\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length, dictionary, numb, method):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        #create the idfv\n",
    "        self.idv(dictionary)\n",
    "        \n",
    "        #create document_term_matrix\n",
    "        self.document_term_matrix(dictionary)\n",
    "        \n",
    "        #create dictionary ranking\n",
    "        self.dict_rank(numb,dictionary, method)\n",
    "        \n",
    "       \n",
    "        #create the tf-idf score\n",
    "        #self.tdidf(self.token_set)\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length < 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "    # exercise 1\n",
    "    def idv(self, dictionary):\n",
    "        \"\"\"\n",
    "        computes the frequency of each term v\n",
    "        \"\"\"\n",
    "        self.idfv = []\n",
    "        for v in list(dictionary):\n",
    "            dfv = 0\n",
    "            for doc in self.docs:\n",
    "                if v in doc.tokens:\n",
    "                    dfv = dfv + 1\n",
    "            if dfv == 0:\n",
    "                self.idfv.append(1000)\n",
    "            else:\n",
    "                c = self.N/dfv\n",
    "                self.idfv.append(math.log10(c)) \n",
    "                \n",
    "\n",
    "    def document_term_matrix(self, dictionary):\n",
    "        \"\"\"\n",
    "        description: create the document_term_matrix\n",
    "        \"\"\"\n",
    "        self.document_term_mat = []\n",
    "        self.tfdv = []\n",
    "        self.tfidf = []\n",
    "        for doc in self.docs:\n",
    "            new = []\n",
    "            tf = []\n",
    "            # count the frequency of each word in the vocabulary for each document\n",
    "            for v in list(dictionary):\n",
    "                temp = list(doc.tokens).count(v)\n",
    "                new.append(temp)\n",
    "                if temp == 0:\n",
    "                    tf.append(temp)\n",
    "                else:\n",
    "                    tf.append(1+ math.log10(temp))\n",
    "            # update both document term matrix and tf_d,v matrix        \n",
    "            self.document_term_mat.append(new)\n",
    "            self.tfdv.append(tf)\n",
    "            self.tfidf.append([a*b for a,b in zip(self.tfdv[-1], self.idfv)])\n",
    "    \n",
    "            \n",
    "    def dict_rank(self, numb, dictionary, method_matrix):\n",
    "        \"\"\"\n",
    "        computes the dictionary rank of the top numb documents, based on the dictionary and method_matrix \n",
    "        (either doc-term or tdidf)\n",
    "        \"\"\"\n",
    "        # compute the score \n",
    "        self.rank = []\n",
    "        if method_matrix == \"doc-term\":\n",
    "            for i in range(self.N):\n",
    "                self.rank.append(sum(self.document_term_mat[i]))\n",
    "        elif method_matrix == \"tdidf\":\n",
    "            for i in range(self.N):\n",
    "                self.rank.append(sum(self.tfidf[i]))\n",
    "        # this tells the order of the ranking of the top numb documents        \n",
    "        self.rank = np.argsort(self.rank) + 1     \n",
    "        self.rank = self.rank[:(numb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fellow-citizens of the s\n"
     ]
    }
   ],
   "source": [
    "#Instantite the corpus class\n",
    "#self, doc_data, stopword_file, clean_length, dictionary, numb, method\n",
    "#corpus = Corpus(pres_speech_list, 'stopwords.txt', 2, corpus.token_set, 3, \"tdidf\")\n",
    "\n",
    "dictionary = [\"positive\", \"joy\", \"optimism\", \"god\"]\n",
    "corpus = Corpus(pres_speech_list, 'stopwords.txt', 3, dictionary, 10, \"tdidf\")\n",
    "print corpus.docs[0].text[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 107, 108, 109, 110, 114, 115, 116, 117, 118])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEMO\n",
    "b = corpus.rank\n",
    "b\n",
    "#corpus.docs[122].pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read two texts with positive and negative words\n",
    "text2 = open('positive.txt', 'r').read()\n",
    "dictionary_pos = text2.splitlines()\n",
    "dictionary_pos = filter(None, dictionary_pos) # define the positive words dictionary\n",
    "text3 = open('negative.txt', 'r').read()\n",
    "dictionary_neg = text3.splitlines() # define the negative words dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus1 = Corpus(pres_speech_list, 'stopwords.txt', 3, dictionary_pos, 10, \"tdidf\")\n",
    "pos_tdidf1 = corpus1.rank\n",
    "pos_tdidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus2 = Corpus(pres_speech_list, 'stopwords.txt', 3, dictionary_pos, 10, \"doc-term\")\n",
    "pos_doc1 = corpus2.rank\n",
    "pos_doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus3 = Corpus(pres_speech_list, 'stopwords.txt', 3, dictionary_neg, 10, \"tdidf\")\n",
    "pos_tdidf2 = corpus3.rank\n",
    "pos_tdidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus4 = Corpus(pres_speech_list, 'stopwords.txt', 3, dictionary_neg, 10, \"doc-term\")\n",
    "pos_doc2 = corpus4.rank\n",
    "pos_doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To create the dictionary\n",
    "afinn = dict(map(lambda (k,v): (k,int(v)), \n",
    "                     [ line.split('\\t') for line in open(\"AFINN-111.txt\") ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
