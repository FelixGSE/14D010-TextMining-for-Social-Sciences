{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\"\"\"\n",
    "This is a class sherlock. \n",
    "Notice how it is defined with the keyword `class` and a name that begins with a capital letter\n",
    "\"\"\"\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text, stopwords, clean_length):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.word_list(clean_length, stopwords)\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "    \n",
    "    def word_list(self, clean_length, stopwords):\n",
    "        \"\"\"\n",
    "        description: define the word_list attribute (i.e. without stemming)\n",
    "        \"\"\"\n",
    "        self.word_list = np.array(wordpunct_tokenize(self.text))\n",
    "        self.word_list = np.array([t for t in self.word_list if (t.isalpha() and len(t) > clean_length)])        \n",
    "        self.word_list = np.array([t for t in self.word_list if t not in stopwords])\n",
    "\n",
    "    def token_clean(self,length):\n",
    "\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "\n",
    "    def stem(self):\n",
    "\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    # We added in the initialization the dictionary with respect to which you want to compute the ranking: \n",
    "    # numb is for the number of top documents you want to consider, while metric == \"tfidf\" or \"doc-term\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "                \n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2], self.stopwords, clean_length) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length < 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "\n",
    "    # Q1 part 1: document_term_matrix - which returns a D by V array of frequency counts.            \n",
    "    def generate_document_term_matrix(self):\n",
    "        \"\"\"\n",
    "        description: create the document_term_matrix\n",
    "        \"\"\"\n",
    "        dimD = self.N\n",
    "        # total number of columns\n",
    "        dimV = len(self.token_set)\n",
    "        terms_list = list(self.token_set)\n",
    "        # initialize the matrix\n",
    "        document_term_matrix = np.zeros((dimD, dimV))        \n",
    "        for i in range(dimD):\n",
    "            # count the terms for each document\n",
    "            document = self.docs[i]\n",
    "            if i%25==0: print 'counting terms for doc: ' + str(i)\n",
    "            word_counts = Counter(document.tokens)\n",
    "            for word_count_pair in word_counts.most_common():\n",
    "                # split in word and count\n",
    "                word = word_count_pair[0]\n",
    "                count = word_count_pair[1]\n",
    "                # save the term index\n",
    "                term_idx = terms_list.index(word)\n",
    "                doc_term_tuple = (i, term_idx)\n",
    "                document_term_matrix.itemset(doc_term_tuple, count)\n",
    "        # update the doc_term_matrix attribute        \n",
    "        self.document_term_matrix = document_term_matrix\n",
    "\n",
    "    def generate_idfv(self):\n",
    "        \"\"\"\n",
    "        computes the inverse document frequency of each term v\n",
    "        \"\"\"\n",
    "        D = self.N\n",
    "        # idf_{v} = log(D/d_fv)\n",
    "        terms_list = list(self.token_set)\n",
    "        self.idfv = dict.fromkeys(terms_list, 0)\n",
    "        # creates a hash {'term':0,'term2':0,...}\n",
    "        for v in self.token_set:\n",
    "            term_idx = terms_list.index(v)\n",
    "            d_fv = np.sum(self.document_term_matrix[:,term_idx] > 0)\n",
    "            # apply the formula\n",
    "            c = math.log10(D/d_fv)\n",
    "            self.idfv[v] = c\n",
    "\n",
    "    # Q1 part 2: tf_idf - returns a D by V array of tf-idf scores            \n",
    "    def generate_tf_idf(self):\n",
    "        D = self.N\n",
    "        terms_list = list(self.token_set)\n",
    "        # initialize the matrix\n",
    "        tf_idf = np.zeros(self.document_term_matrix.shape)\n",
    "        for doc_i in range(D):\n",
    "            if doc_i%25==0: print 'counting terms for doc: ' + str(doc_i)\n",
    "            for v_i in range(len(terms_list)):\n",
    "                doc_term_tuple = (doc_i, v_i)\n",
    "                xdv_score = self.document_term_matrix.item(doc_term_tuple)\n",
    "                if xdv_score > 0:\n",
    "                    idf_score = self.idfv[terms_list[v_i]]\n",
    "                    if idf_score > 0:\n",
    "                        # apply the formula seen in class\n",
    "                        tf_idf_score = (1 + np.log(xdv_score))*idf_score\n",
    "                        tf_idf.itemset(doc_term_tuple, tf_idf_score)\n",
    "        self.tf_idf = tf_idf\n",
    "\n",
    "    # Q1 part 3: dict_rank\n",
    "    def dict_rank(self, numb, dictionary, metric):\n",
    "        \"\"\"\n",
    "        computes the dictionary rank of the top numb documents, based on the dictionary and metric method \n",
    "        (either doc-term or tfidf)\n",
    "        \"\"\"\n",
    "        terms_list = list(self.token_set)\n",
    "        # get the indices of occurences of the terms in the dictionary\n",
    "        idcs = [terms_list.index(item) for item in dictionary]\n",
    "        order = []\n",
    "        if metric == 'doc-term':\n",
    "            # get the needed columns\n",
    "            cols = tuple([list(self.document_term_matrix[:,i]) for i in idcs])\n",
    "            # sort and update order list\n",
    "            order = list(np.lexsort(cols))\n",
    "            order.reverse()\n",
    "        elif metric == 'tf_idf':\n",
    "            cols = tuple([list(self.tf_idf[:,i]) for i in idcs])\n",
    "            order = list(np.lexsort(cols))\n",
    "            order.reverse()\n",
    "        # return the top numb documents    \n",
    "        return [self.docs[i] for i in order[0:numb]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = open('sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fellow-citizens of the s\n"
     ]
    }
   ],
   "source": [
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, 'stopwords.txt', 3)\n",
    "print corpus.docs[0].text[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting terms for doc: 0\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 75\n",
      "counting terms for doc: 100\n",
      "counting terms for doc: 125\n",
      "counting terms for doc: 150\n",
      "counting terms for doc: 175\n",
      "counting terms for doc: 200\n",
      "counting terms for doc: 225\n",
      "time spent computing document_term_matrix: 166.600317001\n",
      "13588\n",
      "counting terms for doc: 0\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 75\n",
      "counting terms for doc: 100\n",
      "counting terms for doc: 125\n",
      "counting terms for doc: 150\n",
      "counting terms for doc: 175\n",
      "counting terms for doc: 200\n",
      "counting terms for doc: 225\n",
      "(236, 13588)\n"
     ]
    }
   ],
   "source": [
    "# Q2 part 1: Use the two methods above to score each document in your data.\n",
    "import time\n",
    "t0 = time.time()\n",
    "corpus.generate_document_term_matrix()\n",
    "corpus.document_term_matrix[:,0]\n",
    "t1 = time.time()\n",
    "print 'time spent computing document_term_matrix: ' + str(t1 - t0)\n",
    "\n",
    "corpus.generate_idfv()\n",
    "print len(corpus.idfv) # == len(corpus.token_set)\n",
    "corpus.idfv[corpus.idfv.keys()[9]]\n",
    "\n",
    "corpus.generate_tf_idf()\n",
    "print corpus.tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Document instance at 0x105723128>, <__main__.Document instance at 0x105723e18>, <__main__.Document instance at 0x10571c5a8>, <__main__.Document instance at 0x105733998>, <__main__.Document instance at 0x105723368>, <__main__.Document instance at 0x10571c248>, <__main__.Document instance at 0x105720200>, <__main__.Document instance at 0x105720368>, <__main__.Document instance at 0x1057235f0>, <__main__.Document instance at 0x1057331b8>]\n",
      "[<__main__.Document instance at 0x105723128>, <__main__.Document instance at 0x10571c5a8>, <__main__.Document instance at 0x105723e18>, <__main__.Document instance at 0x105723368>, <__main__.Document instance at 0x105733998>, <__main__.Document instance at 0x10571c248>, <__main__.Document instance at 0x1057235f0>, <__main__.Document instance at 0x105723098>, <__main__.Document instance at 0x105720998>, <__main__.Document instance at 0x105720368>]\n"
     ]
    }
   ],
   "source": [
    "dictionary = [\"all\",\"children\",\"forget\"]\n",
    "print corpus.dict_rank(10, dictionary, 'doc-term')\n",
    "print corpus.dict_rank(10, dictionary, 'tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read two texts with positive and negative words\n",
    "text2 = open('positive.txt', 'r').read()\n",
    "dictionary_pos = text2.splitlines()\n",
    "dictionary_pos = filter(None, dictionary_pos) # define the positive words dictionary\n",
    "dictionary_pos = np.array([PorterStemmer().stem(t) for t in dictionary_pos])\n",
    "\n",
    "text3 = open('negative.txt', 'r').read()\n",
    "dictionary_neg = text3.splitlines()\n",
    "dictionary_neg = filter(None, dictionary_neg) # define the negative words dictionary\n",
    "dictionary_neg = [re.sub(r'\\W+', '', string) for string in dictionary_neg]\n",
    "dictionary_neg = np.array([PorterStemmer().stem(t) for t in dictionary_neg])  # stemmed and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Q2 part 2: Explore whether the scores diﬀer according to the meta data ﬁelds you gathered:\n",
    "# for example, do diﬀerent speakers/sources/etc tend to receive a higher score than others?\n",
    "pos_tokens = set(dictionary_pos).intersection(corpus.token_set)\n",
    "tdidf_pos = corpus.dict_rank(50, pos_tokens, \"tf_idf\")\n",
    "docterm_pos = corpus.dict_rank(50, pos_tokens, \"doc-term\")\n",
    "\n",
    "neg_tokens = set(dictionary_neg).intersection(corpus.token_set)\n",
    "tdidf_neg = corpus.dict_rank(50, neg_tokens, \"tf_idf\")\n",
    "docterm_neg = corpus.dict_rank(50, neg_tokens, \"doc-term\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Jackson': 6, 'Roosevelt': 5, 'Cleveland': 5, 'Adams': 4, 'Harrison': 4, 'Buren': 3, 'Buchanan': 3, 'McKinley': 3, 'Polk': 3, 'Fillmore': 2, 'Grant': 2, 'Washington': 2, 'Taft': 2, 'Jefferson': 2, 'Taylor': 1, 'Johnson': 1, 'Hayes': 1, 'Coolidge': 1})\n",
      "\n",
      "Counter({'Jackson': 6, 'Roosevelt': 5, 'Cleveland': 5, 'Harrison': 4, 'Adams': 3, 'Buren': 3, 'Buchanan': 3, 'McKinley': 3, 'Polk': 3, 'Fillmore': 2, 'Grant': 2, 'Washington': 2, 'Taft': 2, 'Jefferson': 2, 'Taylor': 1, 'Johnson': 1, 'Hayes': 1, 'Coolidge': 1, 'Monroe': 1})\n",
      "\n",
      "Counter({'Cleveland': 6, 'Jackson': 5, 'Roosevelt': 5, 'Eisenhower': 4, 'Carter': 4, 'Buren': 3, 'Washington': 3, 'Grant': 2, 'Pierce': 2, 'Buchanan': 2, 'Harrison': 2, 'Polk': 2, 'Lincoln': 1, 'Bush': 1, 'Ford': 1, 'Obama': 1, 'Kennedy': 1, 'Nixon': 1, 'Madison': 1, 'McKinley': 1, 'Truman': 1, 'Monroe': 1})\n",
      "\n",
      "Counter({'Carter': 5, 'Eisenhower': 4, 'Roosevelt': 4, 'Cleveland': 4, 'Buren': 3, 'Johnson': 3, 'McKinley': 3, 'Polk': 3, 'Coolidge': 2, 'Jackson': 2, 'Hoover': 2, 'Nixon': 2, 'Taft': 2, 'Truman': 2, 'Wilson': 1, 'Bush': 1, 'Tyler': 1, 'Ford': 1, 'Obama': 1, 'Kennedy': 1, 'Arthur': 1, 'Buchanan': 1, 'Monroe': 1})\n"
     ]
    }
   ],
   "source": [
    "# 50 most positive speeches by td_idf score\n",
    "most_positive_speeches = [doc.pres for doc in tdidf_pos]\n",
    "pos_tdidf_counts = Counter(most_positive_speeches)\n",
    "print pos_tdidf_counts\n",
    "\n",
    "print ''\n",
    "most_positive_speeches = [doc.pres for doc in docterm_pos]\n",
    "pos_docterm_counts = Counter(most_positive_speeches)\n",
    "print pos_docterm_counts\n",
    "\n",
    "print ''\n",
    "most_negative_speeches = [doc.pres for doc in tdidf_neg]\n",
    "neg_tdidf_counts = Counter(most_negative_speeches)\n",
    "print neg_tdidf_counts\n",
    "\n",
    "print ''\n",
    "most_negative_speeches = [doc.pres for doc in docterm_neg]\n",
    "neg_docterm_counts = Counter(most_negative_speeches)\n",
    "print neg_docterm_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2 part 3: Do the answers to the previous question depend on whether tf-idf weighting is applied or not? Why do you think there is (or is not) a diﬀerence in your answers?\n",
    "\n",
    "The answers in the previous question depend on tf-idf weighting. However, it is possibly revealing there is little to no difference in which metric is used for ranking presidents by positive speeches and a greater difference in the negative rankings. This makes sense if you infer from this result presidents use similar positive words but dissimilar negative words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2), ('lurking', -1), ('controversial', -2), ('hating', -3), ('ridiculous', -3), ('hate', -3)]\n"
     ]
    }
   ],
   "source": [
    "# To create the dictionary\n",
    "def valence_scores():\n",
    "    afinn = dict(map(lambda (k,v): (k,int(v)), \n",
    "                         [ line.split('\\t') for line in open(\"AFINN-111.txt\") ]))\n",
    "    return afinn\n",
    "\n",
    "scores = valence_scores()\n",
    "print [(k, scores[k]) for k in scores.keys()[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def speech_sentiment(speech, dictionary, metric):\n",
    "    sentiment_list = []\n",
    "    for word in speech:\n",
    "        temp = dictionary.get( word )\n",
    "        if not temp == None: sentiment_list.append( temp )\n",
    "    output = np.sum(sentiment_list) if metric == 'sum' else np.mean(sentiment_list)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "We added a metric argument to the sentiment score because we found the sentiment scoring subjective: should sentiment be dependent on repetition and length? For example, should a speech made up of words [\"joy\"] have the same sentiment as the speech [\"joy\", \"joy\", \"joy\"], or is the latter more positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76, 131, 125, 80, 62, 48, 94, 176, 60, 89]\n",
      "[1.0857142857142856, 1.6375, 0.94696969696969702, 0.76190476190476186, 0.50819672131147542, 0.2711864406779661, 0.77685950413223137, 1.0731707317073171, 0.49586776859504134, 0.75423728813559321]\n",
      "1981\n",
      "Carter\n",
      "1938\n",
      "Roosevelt\n"
     ]
    }
   ],
   "source": [
    "print [speech_sentiment(doc.word_list, scores, 'sum') for doc in corpus.docs][0:10]\n",
    "print [speech_sentiment(doc.word_list, scores, 'mean') for doc in corpus.docs][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1981\n",
      "Carter\n",
      "1938\n",
      "Roosevelt\n"
     ]
    }
   ],
   "source": [
    "# compute the most and least positive speech\n",
    "rank = np.asarray([speech_sentiment(doc.word_list, scores, 'sum') for doc in corpus.docs])\n",
    "index_pos = rank.argmax()\n",
    "# most positive speech ever\n",
    "print corpus.docs[index_pos].year\n",
    "print corpus.docs[index_pos].pres\n",
    "\n",
    "index_neg = rank.argmin()\n",
    "# least positive speech ever\n",
    "print corpus.docs[index_neg].year\n",
    "print corpus.docs[index_neg].pres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
