{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Oriented Programming - Quick and dirty Introduction\n",
    "\n",
    "In this session, we go through a toy example of object oriented programming in python and then look at the skeleton class of a document. Clearly, this topic is vast and requires a much more through treatment than being given here. But I find it more useful to introduce you to concepts because we need them rather than because they exist. \n",
    "\n",
    "#### Why will we use object oriented programming in this class?\n",
    "Amongst the many many reasons that OOP makes sense, for us the most important one is:\n",
    "\n",
    "1. __Reusability__\n",
    "We want to be able to write all our code for every document once and then call it again and again. Having stand alone functinons not only increases the amount of _house keeping_ but will also require you to change code at different places everytime your requirements change.\n",
    "\n",
    "2. __Encapsulation__\n",
    "Remember that you will have hundreds of documents and will have to define properties for each one of them. For example, how long is every document or what are its tokens. OOP allows you to define all of these properties at one place and then the details of these implementations can be hidden.\n",
    "\n",
    "#### Important Terminology\n",
    "\n",
    "1. __Class__: Is _dna_ or _blueprint_ for an object that has to be modeled. Contains attributes that contins its properties and methods that characterize its behaviors.\n",
    "\n",
    "2. __Instance__: An individual object of a class.\n",
    "\n",
    "3. __Instantiation__: The creation of an instance of a class.\n",
    "\n",
    "4. __Object__: A unique instance of a class. An object comprises both data members (class variables and instance variables) and methods.\n",
    "\n",
    "5. __Class variable__: A variable that is shared by all instances of a class. They are defined within a class but outside any of the class's methods\n",
    "\n",
    "6. __Instance variable__: A variable that is defined inside a method and belongs only to the current instance of a class.\n",
    "\n",
    "7. __Data member__: A class variable or instance variable that holds data associated with a class and its objects.\n",
    "\n",
    "8. __Method__ : A function that is defined in a class definition. \n",
    "\n",
    "So all the chit chat aside lets get down to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "    def token_clean(self,length):\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self\n",
    "Notice the `self` keyword that is present all over the place in the class. `self` tells python which object it needs to work with. \n",
    "\n",
    "1. Therefore every method should have a `self` parameter specified.\n",
    "2. Notice that when the method is called the reference to the object is passed implicitly.\n",
    "3. Every data member of the class needs to be referred to using `self`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Document instance at 0x1084269e0>\n",
      "['this' 'is' 'a' 'chicken']\n",
      "Document instance has no attribute 'demo_self'\n"
     ]
    }
   ],
   "source": [
    "#Instantiating an object.  \n",
    "speech1 = Document('1986', 'Rooster', 'this is a chicken')\n",
    "print speech1\n",
    "\n",
    "#Accessing data members\n",
    "print speech1.tokens\n",
    "\n",
    "try:\n",
    "    speech1.demo_self()\n",
    "except Exception as ex:\n",
    "    print ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of usefulness of classes\n",
    "\n",
    "Use this [link](http://www.codeskulptor.org/#user41_X9owzYCGupNiKYr.py) to see how useful classes can be. The link leads to a code I wrote two years ago that implements a very naive version of the game Asteroids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A skeleton class structure for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\n",
    "class Corpus():\n",
    "    \n",
    "    \"\"\" \n",
    "    The Corpus class represents a document collection\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, doc_data, stopword_file, clean_length):\n",
    "        \"\"\"\n",
    "        Notice that the __init__ method is invoked everytime an object of the class\n",
    "        is instantiated\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        #Initialise documents by invoking the appropriate class\n",
    "        self.docs = [Document(doc[0], doc[1], doc[2]) for doc in doc_data] \n",
    "        \n",
    "        self.N = len(self.docs)\n",
    "        self.clean_length = clean_length\n",
    "        \n",
    "        #get a list of stopwords\n",
    "        self.create_stopwords(stopword_file, clean_length)\n",
    "        \n",
    "        #stopword removal, token cleaning and stemming to docs\n",
    "        self.clean_docs(2)\n",
    "        \n",
    "        #create vocabulary\n",
    "        self.corpus_tokens()\n",
    "        \n",
    "    def clean_docs(self, length):\n",
    "        \"\"\" \n",
    "        Applies stopword removal, token cleaning and stemming to docs\n",
    "        \"\"\"\n",
    "        for doc in self.docs:\n",
    "            doc.token_clean(length)\n",
    "            doc.stopword_remove(self.stopwords)\n",
    "            doc.stem()        \n",
    "    \n",
    "    def create_stopwords(self, stopword_file, length):\n",
    "        \"\"\"\n",
    "        description: parses a file of stowords, removes words of length 'length' and \n",
    "        stems it\n",
    "        input: length: cutoff length for words\n",
    "               stopword_file: stopwords file to parse\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(stopword_file,'r','utf-8') as f: raw = f.read()\n",
    "        \n",
    "        self.stopwords = (np.array([PorterStemmer().stem(word) \n",
    "                                    for word in list(raw.splitlines()) if len(word) > length]))\n",
    "        \n",
    "     \n",
    "    def corpus_tokens(self):\n",
    "        \"\"\"\n",
    "        description: create a set of all all tokens or in other words a vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialise an empty set\n",
    "        self.token_set = set()\n",
    "        for doc in self.docs:\n",
    "            self.token_set = self.token_set.union(doc.tokens) \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document():\n",
    "    \n",
    "    \"\"\" The Doc class rpresents a class of individul documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, speech_year, speech_pres, speech_text):\n",
    "        self.year = speech_year\n",
    "        self.pres = speech_pres\n",
    "        self.text = speech_text.lower()\n",
    "        self.tokens = np.array(wordpunct_tokenize(self.text))\n",
    "        \n",
    "    def token_clean(self,length):\n",
    "        \"\"\" \n",
    "        description: strip out non-alpha tokens and tokens of length > 'length'\n",
    "        input: length: cut off length \n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if (t.isalpha() and len(t) > length)])\n",
    "\n",
    "    def stopword_remove(self, stopwords):\n",
    "        \"\"\"\n",
    "        description: Remove stopwords from tokens.\n",
    "        input: stopwords: a suitable list of stopwords\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([t for t in self.tokens if t not in stopwords])\n",
    "\n",
    "    def stem(self):\n",
    "        \"\"\"\n",
    "        description: Stem tokens with Porter Stemmer.\n",
    "        \"\"\"\n",
    "        self.tokens = np.array([PorterStemmer().stem(t) for t in self.tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the presedential speech dataset to demonstrate how the class works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_text(textraw, regex):\n",
    "    \"\"\"takes raw string and performs two operations\n",
    "    1. Breaks text into a list of speech, president and speech\n",
    "    2. breaks speech into paragraphs\n",
    "    \"\"\"\n",
    "    prs_yr_spch_reg = re.compile(regex, re.MULTILINE|re.DOTALL)\n",
    "    \n",
    "    #Each tuple contains the year, last ane of the president and the speech text\n",
    "    prs_yr_spch = prs_yr_spch_reg.findall(textraw)\n",
    "    \n",
    "    #convert immutabe tuple to mutable list\n",
    "    prs_yr_spch = [list(tup) for tup in prs_yr_spch]\n",
    "    \n",
    "    for i in range(len(prs_yr_spch)):\n",
    "        prs_yr_spch[i][2] = prs_yr_spch[i][2].replace('\\n', '')\n",
    "    \n",
    "    #sort\n",
    "    prs_yr_spch.sort()\n",
    "    \n",
    "    return(prs_yr_spch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('./../data/pres_speech/sou_all.txt', 'r').read()\n",
    "regex = \"_(\\d{4}).*?_[a-zA-Z]+.*?_[a-zA-Z]+.*?_([a-zA-Z]+)_\\*+(\\\\n{2}.*?)\\\\n{3}\"\n",
    "pres_speech_list = parse_text(text, regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Corpus instance at 0x108448998>\n",
      "<__main__.Document instance at 0x1084489e0>\n"
     ]
    }
   ],
   "source": [
    "#Instantite the corpus class\n",
    "corpus = Corpus(pres_speech_list, './../data/stopwords/stopwords.txt', 2)\n",
    "\n",
    "print corpus\n",
    "print corpus.docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc1 = corpus.docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document class data members\n",
      "Washington\n",
      "1790\n",
      "[u'fellow' u'citizen' u'senat' u'hous' u'repres']\n",
      " fellow-citizens of the senate and house of representatives: in meeting you again i feel much satisf\n"
     ]
    }
   ],
   "source": [
    "print 'Document class data members'\n",
    "print doc1.pres\n",
    "print doc1.year\n",
    "print doc1.tokens[0:5]\n",
    "print doc1.text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(236, 13568)\n",
      "counting terms for doc: 0\n",
      "counting terms for doc: 1\n",
      "counting terms for doc: 2\n",
      "counting terms for doc: 3\n",
      "counting terms for doc: 4\n",
      "counting terms for doc: 5\n",
      "counting terms for doc: 6\n",
      "counting terms for doc: 7\n",
      "counting terms for doc: 8\n",
      "counting terms for doc: 9\n",
      "counting terms for doc: 10\n",
      "counting terms for doc: 11\n",
      "counting terms for doc: 12\n",
      "counting terms for doc: 13\n",
      "counting terms for doc: 14\n",
      "counting terms for doc: 15\n",
      "counting terms for doc: 16\n",
      "counting terms for doc: 17\n",
      "counting terms for doc: 18\n",
      "counting terms for doc: 19\n",
      "counting terms for doc: 20\n",
      "counting terms for doc: 21\n",
      "counting terms for doc: 22\n",
      "counting terms for doc: 23\n",
      "counting terms for doc: 24\n",
      "counting terms for doc: 25\n",
      "counting terms for doc: 26\n",
      "counting terms for doc: 27\n",
      "counting terms for doc: 28\n",
      "counting terms for doc: 29\n",
      "counting terms for doc: 30\n",
      "counting terms for doc: 31\n",
      "counting terms for doc: 32\n",
      "counting terms for doc: 33\n",
      "counting terms for doc: 34\n",
      "counting terms for doc: 35\n",
      "counting terms for doc: 36\n",
      "counting terms for doc: 37\n",
      "counting terms for doc: 38\n",
      "counting terms for doc: 39\n",
      "counting terms for doc: 40\n",
      "counting terms for doc: 41\n",
      "counting terms for doc: 42\n",
      "counting terms for doc: 43\n",
      "counting terms for doc: 44\n",
      "counting terms for doc: 45\n",
      "counting terms for doc: 46\n",
      "counting terms for doc: 47\n",
      "counting terms for doc: 48\n",
      "counting terms for doc: 49\n",
      "counting terms for doc: 50\n",
      "counting terms for doc: 51\n",
      "counting terms for doc: 52\n",
      "counting terms for doc: 53\n",
      "counting terms for doc: 54\n",
      "counting terms for doc: 55\n",
      "counting terms for doc: 56\n",
      "counting terms for doc: 57\n",
      "counting terms for doc: 58"
     ]
    }
   ],
   "source": [
    "dimD = len(corpus.docs)\n",
    "dimV = len(corpus.token_set)\n",
    "# record arrays are not our friends\n",
    "# so we use a dict to store term - document term matrix indices\n",
    "terms_arr = [str(term) for term in corpus.token_set]\n",
    "#print(terms_arr)\n",
    "arr = np.zeros((dimD, dimV))\n",
    "document_term_matrix = np.array(arr)\n",
    "print document_term_matrix.shape\n",
    "\n",
    "for i in range(len(corpus.docs)):\n",
    "    document = corpus.docs[i]\n",
    "    print 'counting terms for doc: ' + str(i)\n",
    "    for token in document.tokens:\n",
    "        # get document term index for the word\n",
    "        term_idx = terms_arr.index(token)\n",
    "        doc_term_tuple = (i, term_idx)\n",
    "        document_term_matrix.itemset(doc_term_tuple, document_term_matrix.item(doc_term_tuple) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW 1, pt 1\n",
    "# document_term_matrix - which returns a D by V array of frequency counts.\n",
    "# will go in the corpus class which already has such things.\n",
    "def document_term_matrix(corpus):\n",
    "    # initiate an empty list to store dictionaries for each term\n",
    "    # magnitude of V\n",
    "    dimV = len(corpus.token_set)\n",
    "    dimD = len(corpus.docs)\n",
    "    # Assumption: terms == tokens, not clear to me if this is the case\n",
    "    document_term_matrix = np.array([[0]*dimD*]dimV, dtype = [(term, 'a25') for term in corpus.corpus_tokens])\n",
    "    for i in range(len(documents)):\n",
    "        document = self.documents[i]\n",
    "        \n",
    "        for word in range(len(document.words)):\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
